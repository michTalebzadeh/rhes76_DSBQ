from __future__ import print_function
import sys
import signal
import json

# Import necessary modules and add paths
sys.path.append('/home/hduser/dba/bin/python/DSBQ/')
sys.path.append('/home/hduser/dba/bin/python/DSBQ/conf')
sys.path.append('/home/hduser/dba/bin/python/DSBQ/othermisc')
sys.path.append('/home/hduser/dba/bin/python/DSBQ/src')
sys.path.append('/home/hduser/.local/lib/python3.9/site-packages')
import findspark
findspark.init()
from config import config
from pyspark.sql import functions as F
from pyspark.sql.functions import col, round, current_timestamp, lit
from pyspark.sql.types import StructType, StringType, IntegerType, FloatType, TimestampType
from pyspark.sql.window import Window
from sparkutils import sparkstuff as s
from othermisc import usedFunctions as uf
import datetime
import time
import uuid
from pyspark.sql.streaming import DataStreamWriter
from pyspark.sql.functions import udf
import schedule
from flask import Flask, jsonify, request
from flask.signals import got_request_exception
from flask_restful import Resource, Api
from pyspark.sql.streaming import DataStreamWriter
import socket
from pyspark.sql import SparkSession
from pyspark import SparkContext

# Declare Flask app and Api
app = Flask(__name__)
api = Api(app)


def main():
    # Set the application name
    appName = "RandomDataPysparkStreaming"

    # Initialize Spark session and context
    spark_session = s.spark_session(appName)
    spark_session = s.setSparkConfStreaming(spark_session)
    spark_session = s.setSparkConfBQ(spark_session)
    spark_context = spark_session.sparkContext
    #spark_context = s.sparkcontext()
    
    # Create an instance of RandomDataResource class
    random_resource = RandomDataResource(spark_session)

    # Set the log level to ERROR to reduce verbosity
    spark_context.setLogLevel("ERROR")

    # Get start time
    lst = (spark_session.sql("SELECT FROM_unixtime(unix_timestamp(), 'dd/MM/yyyy HH:mm:ss.ss') ")).collect()
    print("\nStarted at")
    uf.println(lst)
    # Pass the SparkSession to the Flask app
    app.spark_session = spark_session
 
    # Attach the signal for Flask app shutdown
    got_request_exception.connect(stop_spark_session, app)

    # Run the Flask app on your desired hostname and port
    hostname = socket.gethostname()
    app.run(debug=True, host=hostname, port=7999)
  
    
class RandomDataResource(Resource):
     
    def __init__(self, spark_session=None):
        super(RandomDataResource, self).__init__()
        if not SparkContext._active_spark_context:
            # Create a new SparkContext if it doesn't exist
            self.spark_context = SparkContext(appName)
        else:
            # Use the existing SparkContext
            self.spark_context = SparkContext.getOrCreate()

        self.spark = spark_session or SparkSession.builder.appName("YourAppName").getOrCreate()
        self.checkpoint_path = "file:///ssd/hduser/randomdata/chkpt"
        self.result_query = None  # Initialize the StreamingQuery object

    def handle_interrupt(self, signum, frame):
        print("Received Ctrl+C. Stopping streaming gracefully.")
        # Stop the streaming query if it is running
        if self.result_query is not None:
            self.result_query.stop()
        sys.exit(0)  # Exit the program gracefully

    def start_streaming(self, df, appName, numRows, processingTime):
        print("Inside start_streaming \n")
        df.printShema()
        self.result_query = df.writeStream \
            .outputMode('append') \
            .option("truncate", "false") \
            .foreachBatch(lambda df, batchId: self.rates(df, batchId)) \
            .trigger(processingTime=f'{processingTime} seconds') \
            .option('checkpointLocation', self.checkpoint_path) \
            .queryName(f"{appName}") \
            .start()  
       
    def padSingleChar2(self, chars, length):
        # Pad a single character to a specified length
        result_str = F.concat_ws("", F.sequence(F.lit(1), F.lit(length), F.lit(1)), F.lit(chars).cast("string"))
        return result_str

    def rates(self, df: F.DataFrame, batchId: int) -> None:
      if(len(df.take(1))) > 0:
        print("--> I am in rate")
        df.select(col("timestamp"), col("value"), col("rowkey"), col("ID"), col("CLUSTERED"), col("op_time")).show(1, False)
      else:
        print("DataFrame is empty")

    def generate_random_data(self, numRows):
        # ... (remaining code)
        numRows = 10
        processingTime = 2
        rows = 0
        values = self.getValuesFromBQTable()
        rows = values["rows"]
        maxID = values["maxID"]
        start = 0
        if rows == 0:
            start = 1
        else:
            start = maxID + 1
        end = start + numRows
        print("starting at ID = ", start, ",ending on = ", end)
        Range = range(start, end)
        # Kafka producer requires a key, value pair. We generate UUID key as the unique identifier of Kafka record
        # # This traverses through the Range and increment "x" by one unit each time, and that x value is used in the code to generate random data through Python functions in a class

        rdd = self.sc.parallelize(Range). \
            map(lambda x: (str(uuid.uuid4()),
                           x, \
                           uf.clustered(x, numRows), \
                           uf.scattered(x, numRows),
                           uf.randomised(x, numRows), \
                           uf.randomString(50),
                           uf.padString(x, " ", 50), \
                           uf.padSingleChar("x", 50)))
        # Convert RDD to DataFrame
        df = rdd.toDF(["KEY", "ID", "CLUSTERED", "SCATTERED", "RANDOMISED", "RANDOM_STRING", "SMALL_VC", "PADDING"])

        # Add metadata columns
        df = df.withColumn("op_type", lit(config['MDVariables']['op_type'])). \
            withColumn("op_time", current_timestamp())

        # define checkpoint directory
        checkpoint_path = "file:///ssd/hduser/randomdata/chkpt"

        def handle_interrupt(signum, frame):
           print("Received Ctrl+C. Stopping streaming gracefully.")
           # Perform cleanup or stopping logic here
           # sys.exit(0)

        # Set up a signal handler for SIGINT (Ctrl+C)
        signal.signal(signal.SIGINT, random_resource.handle_interrupt)
    
        try:

            # construct a streaming dataframe that subscribes to topic rate for data
            streamingDataFrame = self.spark \
                .readStream \
                .format("rate") \
                .option("rowsPerSecond", numRows) \
                .option("auto.commit.interval.ms", config['MDVariables']['autoCommitIntervalMS']) \
                .option("subscribe", "rate") \
                .option("failOnDataLoss", "false") \
                .option("includeHeaders", "true") \
                .option("startingOffsets", "latest") \
                .load() \
                .withColumn("KEY", F.lit(str(uuid.uuid4()))) \
                .withColumn("ID", F.col("value")) \
                .withColumn("CLUSTERED", uf.clustered2(F.col("value"), numRows)) \
                .withColumn("SCATTERED", F.abs((F.col("value") - 1) % numRows) * 1.0) \
                .withColumn("RANDOMISED", uf.randomised2(F.col("value"), numRows)) \
                .withColumn("RANDOM_STRING", udf(uf.randomString, StringType())(F.lit(50))) \
                .withColumn("SMALL_VC", uf.padString2(col("value") + 1, 50, " ")) \
                .withColumn("PADDING", self.padSingleChar2(lit("x"), lit(50))) \
                .withColumn("op_type", lit(config['MDVariables']['op_type'])) \
                .withColumn("op_time", F.current_timestamp())

            """
            In the context of Spark Structured Streaming with the "rate" source,
            the schema of the streaming DataFrame includes two columns:

            - timestamp: This column contains the timestamp associated with each generated record. It represents the time at which the record was generated.
            - value: This column contains a long integer value associated with each record.          
            """
                     
           # 
           # streamingDataFrame.printSchema()
        
            appName = "RandomDataPysparkStreaming"
            result = streamingDataFrame.select(
                    col("timestamp").alias("timestamp"),
                    col("value").alias("value"),
                    col("KEY").alias("rowkey"),
                    col("ID").alias("ID"),
                    col("CLUSTERED").alias("CLUSTERED"),
                    col("RANDOMISED").alias("RANDOMISED"),
                    col("RANDOM_STRING").alias("RANDOM_STRING"),
                    col("SMALL_VC").alias("SMALL_VC"),
                    col("PADDING").alias("PADDING"),
                    col("RANDOM_STRING").alias("RANDOM_STRING"),
                    col("op_type").alias("op_type"),
                    col("op_time").alias("op_time")
                )

            result_query = (
                    result.writeStream
                    .outputMode('append')
                    .option("truncate", "false")
                    .foreachBatch(lambda df, batchId: self.rates(df, batchId))
                    .trigger(processingTime=f'{processingTime} seconds')
                    .option('checkpointLocation', checkpoint_path)
                    .queryName(f"{appName}")
                    .start()
                )

            # Wait for the query to terminate (you may want to handle this differently in a production setting)
            result_query.awaitTermination()
        except Exception as e:
            print(f"{str(e)}, quitting!")
            sys.exit(1)

    def get(self):
        # Handle the GET request here
        return self.generate_random_data(numRows=10)

    def create_streaming_dataframe(self, numRows):
        return self.spark.readStream.format("rate") \
            .option("rowsPerSecond", numRows) \
            .option("auto.commit.interval.ms", config['MDVariables']['autoCommitIntervalMS']) \
            .option("subscribe", "rate") \
            .option("failOnDataLoss", "false") \
            .option("includeHeaders", "true") \
            .option("startingOffsets", "latest") \
            .load() \
            .withColumn("KEY", lit(str(uuid.uuid4()))) \
            .withColumn("ID", col("value")) \
            .withColumn("CLUSTERED", uf.clustered2(col("value"), numRows)) \
            .withColumn("SCATTERED", (col("value") - 1) % numRows * 1.0) \
            .withColumn("RANDOMISED", uf.randomised2(col("value"), numRows)) \
            .withColumn("RANDOM_STRING", udf(uf.randomString, StringType())(lit(50))) \
            .withColumn("SMALL_VC", uf.padString2(col("value") + 1, 50, " ")) \
            .withColumn("PADDING", uf.padSingleChar2(lit("x"), lit(50))) \
            .withColumn("op_type", lit(config['MDVariables']['op_type'])) \
            .withColumn("op_time", current_timestamp())

    def start_streaming(self, df, appName, numRows, processingTime):
        print ("inside start_streaming \n")
        df.printShema()
        df.writeStream \
            .outputMode('append') \
            .option("truncate", "false") \
            .foreachBatch(lambda df, batchId: self.rates(df, batchId)) \
            .trigger(processingTime=f'{processingTime} seconds') \
            .option('checkpointLocation', self.checkpoint_path) \
            .queryName(f"{appName}") \
            .start()
  
    def getValuesFromBQTable(self):
        # Get row count and max ID from a BigQuery table
        read_df = self.readDataFromBQTable()
        read_df.createOrReplaceTempView("tmp_view")
        rows = self.spark.sql("SELECT COUNT(1) FROM tmp_view").collect()[0][0]
        maxID = self.spark.sql("SELECT MAX(ID) FROM tmp_view").collect()[0][0]
        return {"rows": rows, "maxID": maxID}
    
    def readDataFromBQTable(self):
        # Read data from a BigQuery table
        dataset = "test"
        tableName = "randomdata"
        fullyQualifiedTableName = dataset + '.' + tableName
        read_df = s.loadTableFromBQ(self.spark, dataset, tableName)
        return read_df

    def main(self, numRows=10, processingTime=2):
        dfRandom = self.generate_random_data(numRows)
        streamingDataFrame = self.create_streaming_dataframe(numRows)
        self.start_streaming(streamingDataFrame, "RandomDataPysparkStreaming", numRows, processingTime)

# End of class block


# API endpoints
api.add_resource(RandomDataResource, '/generate_random_data')


# Handle root path
@app.route('/')
def home():
    return 'Welcome to the Random Data Streaming API with Flask!'


# Handle favicon.ico request
@app.route('/favicon.ico')
def favicon():
    # You can return an actual favicon file or just a placeholder response
    return ''


def stop_spark_session(sender, exception, **extra):
    # Stop the Spark session when the Flask app is shutting down
    if sender and hasattr(sender, 'spark_session'):
        sender.spark_session.stop()
        if hasattr(sender, 'result_query'):
            sender.result_query.stop()


def get_or_create_spark_session(appName):
    try:
        # If a SparkSession already exists, use it
        return SparkSession.builder.appName(appName).getOrCreate()
    except Exception as e:
        print(f"Error getting or creating SparkSession: {str(e)}")
        raise  


if __name__ == '__main__':
   main()
